
R version 2.9.0 alpha (2009-03-22 r48193)
Copyright (C) 2009 The R Foundation for Statistical Computing
ISBN 3-900051-07-0

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> library('ipred')
Loading required package: rpart
Loading required package: MASS
Loading required package: mlbench
Loading required package: survival
Loading required package: splines
Loading required package: nnet
Loading required package: class
> 
> actversion <- paste(R.version$major, R.version$minor, sep=".")
> thisversion <- "1.7.0"
> 
> #if (compareVersion(actversion, thisversion) >= 0) {
> #  RNGversion("1.6.2")
> #}
> set.seed(29081975)
> 
> 
> # Classification
> 
> learn <- as.data.frame(mlbench.twonorm(200))
> test <- as.data.frame(mlbench.twonorm(100))
> 
> # bagging
> 
> mod <- bagging(classes ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of misclassification error:  0.185 

> predict(mod)[1:10]
 [1] 1 1 1 1 2 2 1 1 1 1
Levels: 1 2
> 
> # Double-Bagging
> 
> comb.lda <- list(list(model=lda, predict=function(obj, newdata)
+                       predict(obj, newdata)$x))
> 
> mod <- bagging(classes ~ ., data=learn, comb=comb.lda, nbagg=10)
> mod

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = classes ~ ., data = learn, comb = comb.lda, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
 [1] 1 1 1 1 2 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="aver")
 [1] 1 1 1 1 2 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], agg="wei")
 [1] 1 1 1 1 1 2 1 2 1 2
Levels: 1 2
> predict(mod, newdata=test[1:10,], type="prob")
        1   2
 [1,] 1.0 0.0
 [2,] 0.9 0.1
 [3,] 1.0 0.0
 [4,] 1.0 0.0
 [5,] 0.5 0.5
 [6,] 0.0 1.0
 [7,] 1.0 0.0
 [8,] 0.0 1.0
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="aver")
        1   2
 [1,] 1.0 0.0
 [2,] 0.9 0.1
 [3,] 1.0 0.0
 [4,] 1.0 0.0
 [5,] 0.5 0.5
 [6,] 0.0 1.0
 [7,] 1.0 0.0
 [8,] 0.0 1.0
 [9,] 1.0 0.0
[10,] 0.0 1.0
> predict(mod, newdata=test[1:10,], type="prob", agg="wei")
              1           2
 [1,] 1.0000000 0.000000000
 [2,] 0.9976303 0.002369668
 [3,] 1.0000000 0.000000000
 [4,] 1.0000000 0.000000000
 [5,] 0.5311111 0.468888889
 [6,] 0.0000000 1.000000000
 [7,] 1.0000000 0.000000000
 [8,] 0.0000000 1.000000000
 [9,] 1.0000000 0.000000000
[10,] 0.0000000 1.000000000
> 
> mypredict.lda <- function(object, newdata)
+        predict(object, newdata = newdata)$class
> 
> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda)

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.04 

> errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda,
+   est.para=control.errorest(k=5, random=FALSE))

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, est.para = control.errorest(k = 5, 
        random = FALSE))

	 5-fold cross-validation estimator of misclassification error 

Misclassification error:  0.04 

> 
> lapply(errorest(classes ~ ., data=learn, model=lda, predict=mypredict.lda,
+   est.para=control.errorest(k=5, random=FALSE, getmodels=TRUE))$models, class)
[[1]]
[1] "lda"

[[2]]
[1] "lda"

[[3]]
[1] "lda"

[[4]]
[1] "lda"

[[5]]
[1] "lda"

> errorest(classes ~ ., data=learn, model=bagging,
+          est.para=control.errorest(k=2), nbagg=10)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = control.errorest(k = 2), nbagg = 10)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.095 

> errorest(classes ~ ., data=learn, model=bagging,
+          est.para=control.errorest(k=2), nbagg=10, comb=comb.lda)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = bagging, 
    est.para = control.errorest(k = 2), nbagg = 10, comb = comb.lda)

	 2-fold cross-validation estimator of misclassification error 

Misclassification error:  0.085 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="boot")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "boot")

	 Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.0391 
Standard deviation: 0.0027 

> errorest(classes ~ ., data=learn, model=lda,
+ predict=mypredict.lda, estimator="632plus")

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = lda, 
    predict = mypredict.lda, estimator = "632plus")

	 .632+ Bootstrap estimator of misclassification error 
	 with 25 bootstrap replications

Misclassification error:  0.0333 

> 
> # Regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> # bagging
> 
> mod <- bagging(y ~ ., data=learn, coob=TRUE, nbagg=10)
> mod

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, coob = TRUE, 
    nbagg = 10)

Out-of-bag estimate of root mean squared error:  3.5394 

> predict(mod)[1:10]
 [1] 17.52261 10.07663 12.96417 13.86483 14.07599 16.60311       NA 21.18277
 [9] 16.92367       NA
> 
> predict(mod, newdata=test[1:10,])
 [1] 17.988182 11.996216 19.349240 16.608268 11.051775 15.140120 10.996003
 [8]  9.471202 14.869813 13.846185
> predict(mod, newdata=test[1:10,], agg="aver") 
 [1] 17.988182 11.996216 19.349240 16.608268 11.051775 15.140120 10.996003
 [8]  9.471202 14.869813 13.846185
> predict(mod, newdata=test[1:10,], agg="wei")  
 [1] 18.163683 12.034196 19.291462 16.861436 11.071367 15.258870 11.600688
 [8]  9.460784 14.971003 13.922154
> errorest(y ~ ., data=learn, model=lm)

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm)

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.6627 

> errorest(y ~ ., data=learn, model=lm,
+          est.para=control.errorest(k=5, random=FALSE))

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    est.para = control.errorest(k = 5, random = FALSE))

	 5-fold cross-validation estimator of root mean squared error

Root mean squared error:  2.5703 

> lapply(errorest(y ~ ., data=learn, model=lm,
+                 est.para=control.errorest(k=5, random=FALSE, getmodels=TRUE))$models, class)
[[1]]
[1] "lm"

[[2]]
[1] "lm"

[[3]]
[1] "lm"

[[4]]
[1] "lm"

[[5]]
[1] "lm"

> errorest(y ~ ., data=learn, model=lm, estimator="boot")

Call:
errorest.data.frame(formula = y ~ ., data = learn, model = lm, 
    estimator = "boot")

	 Bootstrap estimator of root mean squared error 
	 with 25 bootstrap replications

Root mean squared error:  2.6721 

> 
> # survival
> 
> learn <- rsurv(100, model="C")
> test <- rsurv(100, model="C")
> 
> mod <- bagging(Surv(time, cens) ~ ., data=learn, nbagg=10)
> mod

Bagging survival trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    nbagg = 10)


> predict(mod, newdata=test[1:10,])
[[1]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
183.0000 183.0000 183.0000 183.0000   0.0421   0.0342   0.0475 

[[2]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

records   n.max n.start  events  median 0.95LCL 0.95UCL 
130.000 130.000 130.000 130.000   0.501   0.489   0.532 

[[3]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
160.0000 160.0000 160.0000 160.0000   0.0421   0.0359   0.0568 

[[4]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
152.0000 152.0000 152.0000 152.0000   0.0475   0.0378   0.0514 

[[5]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

records   n.max n.start  events  median 0.95LCL 0.95UCL 
130.000 130.000 130.000 130.000   0.489   0.399   0.532 

[[6]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
149.0000 149.0000 149.0000 149.0000   0.0302   0.0172   0.0342 

[[7]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

records   n.max n.start  events  median 0.95LCL 0.95UCL 
141.000 141.000 141.000 141.000   0.489   0.398   0.532 

[[8]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
160.0000 160.0000 160.0000 160.0000   0.0315   0.0187   0.0359 

[[9]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

records   n.max n.start  events  median 0.95LCL 0.95UCL 
169.000 169.000 169.000 169.000   0.175   0.137   0.209 

[[10]]
Call: survfit(formula = Surv(agglsample[[j]], aggcens[[j]]) ~ 1)

 records    n.max  n.start   events   median  0.95LCL  0.95UCL 
132.0000 132.0000 132.0000 132.0000   0.1050   0.0718   0.1191 

> 
> #errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         est.para=list(k=2, random=FALSE), nbagg=5)
> #errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         estimator="boot", nbagg=5, est.para=list(nboot=5))
> #insert control.errorest
> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          est.para=control.errorest(k=2, random=FALSE), nbagg=5)

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, est.para = control.errorest(k = 2, random = FALSE), 
    nbagg = 5)

	 2-fold cross-validation estimator of Brier's score

Brier's score:  0.0955 

> errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          estimator="boot", nbagg=5, est.para=control.errorest(nboot=5))

Call:
errorest.data.frame(formula = Surv(time, cens) ~ ., data = learn, 
    model = bagging, estimator = "boot", est.para = control.errorest(nboot = 5), 
    nbagg = 5)

	 Bootstrap estimator of Brier's score
	 with 5 bootstrap replications

Brier's score:  0.069 

> 
> #lapply(errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
> #         estimator="cv", nbagg=1, est.para=list(k=2, random=FALSE,
> #         getmodels=TRUE))$models, class)
> #insert control.errorest
> lapply(errorest(Surv(time, cens) ~ ., data=learn, model=bagging, 
+          estimator="cv", nbagg=1, est.para=control.errorest(k=2, random=FALSE,
+          getmodels=TRUE))$models, class)
[[1]]
[1] "survbagg"

[[2]]
[1] "survbagg"

> 
> # bundling for regression
> 
> learn <- as.data.frame(mlbench.friedman1(100))
> test <- as.data.frame(mlbench.friedman1(100))
> 
> comb <- list(list(model=lm, predict=predict.lm))
> 
> modc <- bagging(y ~ ., data=learn, nbagg=10, comb=comb)
> modc

Bagging regression trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, nbagg = 10, 
    comb = comb)


> predict(modc, newdata=learn)[1:10]
 [1]  6.440993 14.435812 10.847327 14.837533 13.010493  6.156936 20.132612
 [8]  6.879375 14.076470 16.636056
> 
> # bundling for survival
> 
> while(FALSE) {
+ data(GBSG2)
+ rcomb <- list(list(model=coxph, predict=predict))
+ 
+ mods <- bagging(Surv(time,cens) ~ ., data=GBSG2, nbagg=10, 
+                 comb=rcomb,  control=rpart.control(xval=0))
+ predict(mods, newdata=GBSG2[1:3,])
+ 
+ # test for method dispatch on integer valued responses
+ y <- sample(1:100, 100)
+ class(y)
+ x <- matrix(rnorm(100*5), ncol=5)
+ mydata <- as.data.frame(cbind(y, x))
+ 
+ cv(y, y ~ ., data=mydata, model=lm, predict=predict)
+ bootest(y, y ~ ., data=mydata, model=lm, predict=predict)
+ bagging(y ~., data=mydata, nbagg=10)
+ }
> 
