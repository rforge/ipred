
R version 3.3.2 (2016-10-31) -- "Sincere Pumpkin Patch"
Copyright (C) 2016 The R Foundation for Statistical Computing
Platform: x86_64-pc-linux-gnu (64-bit)

R is free software and comes with ABSOLUTELY NO WARRANTY.
You are welcome to redistribute it under certain conditions.
Type 'license()' or 'licence()' for distribution details.

  Natural language support but running in an English locale

R is a collaborative project with many contributors.
Type 'contributors()' for more information and
'citation()' on how to cite R or R packages in publications.

Type 'demo()' for some demos, 'help()' for on-line help, or
'help.start()' for an HTML browser interface to help.
Type 'q()' to quit R.

> pkgname <- "ipred"
> source(file.path(R.home("share"), "R", "examples-header.R"))
> options(warn = 1)
> library('ipred')
> 
> base::assign(".oldSearch", base::search(), pos = 'CheckExEnv')
> cleanEx()
> nameEx("DLBCL")
> ### * DLBCL
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: DLBCL
> ### Title: Diffuse Large B-Cell Lymphoma
> ### Aliases: DLBCL
> ### Keywords: datasets
> 
> ### ** Examples
> 
> 
> set.seed(290875)
> 
> data("DLBCL", package="ipred")
> library("survival")
> survfit(Surv(time, cens) ~ 1, data=DLBCL)
Call: survfit(formula = Surv(time, cens) ~ 1, data = DLBCL)

      n  events  median 0.95LCL 0.95UCL 
   40.0    22.0    36.0    15.5      NA 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("GlaucomaMVF")
> ### * GlaucomaMVF
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: GlaucomaMVF
> ### Title: Glaucoma Database
> ### Aliases: GlaucomaMVF
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D data("GlaucomaMVF", package = "ipred")
> ##D library("rpart")
> ##D 
> ##D response <- function (data) {
> ##D   attach(data)
> ##D   res <- ifelse((!is.na(clv) & !is.na(lora) & clv >= 5.1 & lora >= 
> ##D         49.23372) | (!is.na(clv) & !is.na(lora) & !is.na(cs) & 
> ##D         clv < 5.1 & lora >= 58.55409 & cs < 1.405) | (is.na(clv) & 
> ##D         !is.na(lora) & !is.na(cs) & lora >= 58.55409 & cs < 1.405) | 
> ##D         (!is.na(clv) & is.na(lora) & cs < 1.405), 0, 1)
> ##D   detach(data)
> ##D   factor (res, labels = c("glaucoma", "normal"))
> ##D }
> ##D 
> ##D errorest(Class~clv+lora+cs~., data = GlaucomaMVF, model=inclass, 
> ##D        estimator="cv", pFUN = list(list(model = rpart)), cFUN = response)
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("bagging")
> ### * bagging
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: bagging
> ### Title: Bagging Classification, Regression and Survival Trees
> ### Aliases: bagging ipredbagg ipredbagg.factor ipredbagg.integer
> ###   ipredbagg.numeric ipredbagg.Surv ipredbagg.default bagging.data.frame
> ###   bagging.default
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
> library("MASS")
> library("survival")
> 
> # Classification: Breast Cancer data
> 
> data("BreastCancer", package = "mlbench")
> 
> # Test set error bagging (nbagg = 50): 3.7% (Breiman, 1998, Table 5)
> 
> mod <- bagging(Class ~ Cl.thickness + Cell.size
+                 + Cell.shape + Marg.adhesion   
+                 + Epith.c.size + Bare.nuclei   
+                 + Bl.cromatin + Normal.nucleoli
+                 + Mitoses, data=BreastCancer, coob=TRUE)
> print(mod)

Bagging classification trees with 25 bootstrap replications 

Call: bagging.data.frame(formula = Class ~ Cl.thickness + Cell.size + 
    Cell.shape + Marg.adhesion + Epith.c.size + Bare.nuclei + 
    Bl.cromatin + Normal.nucleoli + Mitoses, data = BreastCancer, 
    coob = TRUE)

Out-of-bag estimate of misclassification error:  0.0381 

> 
> # Test set error bagging (nbagg=50): 7.9% (Breiman, 1996a, Table 2)
> data("Ionosphere", package = "mlbench")
> Ionosphere$V2 <- NULL # constant within groups
> 
> bagging(Class ~ ., data=Ionosphere, coob=TRUE)

Bagging classification trees with 25 bootstrap replications 

Call: bagging.data.frame(formula = Class ~ ., data = Ionosphere, coob = TRUE)

Out-of-bag estimate of misclassification error:  0.0912 

> 
> # Double-Bagging: combine LDA and classification trees
> 
> # predict returns the linear discriminant values, i.e. linear combinations
> # of the original predictors
> 
> comb.lda <- list(list(model=lda, predict=function(obj, newdata)
+                                  predict(obj, newdata)$x))
> 
> # Note: out-of-bag estimator is not available in this situation, use
> # errorest
> 
> mod <- bagging(Class ~ ., data=Ionosphere, comb=comb.lda) 
> 
> predict(mod, Ionosphere[1:10,])
 [1] good bad  good bad  good bad  good bad  good bad 
Levels: bad good
> 
> # Regression:
> 
> data("BostonHousing", package = "mlbench")
> 
> # Test set error (nbagg=25, trees pruned): 3.41 (Breiman, 1996a, Table 8)
> 
> mod <- bagging(medv ~ ., data=BostonHousing, coob=TRUE)
> print(mod)

Bagging regression trees with 25 bootstrap replications 

Call: bagging.data.frame(formula = medv ~ ., data = BostonHousing, 
    coob = TRUE)

Out-of-bag estimate of root mean squared error:  4.0618 

> 
> library("mlbench")
> learn <- as.data.frame(mlbench.friedman1(200))
> 
> # Test set error (nbagg=25, trees pruned): 2.47 (Breiman, 1996a, Table 8)
> 
> mod <- bagging(y ~ ., data=learn, coob=TRUE)
> print(mod)

Bagging regression trees with 25 bootstrap replications 

Call: bagging.data.frame(formula = y ~ ., data = learn, coob = TRUE)

Out-of-bag estimate of root mean squared error:  2.8532 

> 
> # Survival data
> 
> # Brier score for censored data estimated by 
> # 10 times 10-fold cross-validation: 0.2 (Hothorn et al,
> # 2002)
> 
> data("DLBCL", package = "ipred")
> mod <- bagging(Surv(time,cens) ~ MGEc.1 + MGEc.2 + MGEc.3 + MGEc.4 + MGEc.5 +
+                                  MGEc.6 + MGEc.7 + MGEc.8 + MGEc.9 +
+                                  MGEc.10 + IPI, data=DLBCL, coob=TRUE)
> 
> print(mod)

Bagging survival trees with 25 bootstrap replications 

Call: bagging.data.frame(formula = Surv(time, cens) ~ MGEc.1 + MGEc.2 + 
    MGEc.3 + MGEc.4 + MGEc.5 + MGEc.6 + MGEc.7 + MGEc.8 + MGEc.9 + 
    MGEc.10 + IPI, data = DLBCL, coob = TRUE)

Out-of-bag estimate of Brier's score:  0.2098 

> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’, ‘package:survival’, ‘package:MASS’

> nameEx("dystrophy")
> ### * dystrophy
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: dystrophy
> ### Title: Detection of muscular dystrophy carriers.
> ### Aliases: dystrophy
> ### Keywords: datasets
> 
> ### ** Examples
> 
> ## Not run: 
> ##D 
> ##D data("dystrophy")
> ##D library("rpart")
> ##D errorest(Class~CK+H~AGE+PK+LD, data = dystrophy, model = inbagg, 
> ##D pFUN = list(list(model = lm, predict = mypredict.lm), list(model = rpart)), 
> ##D ns = 0.75, estimator = "cv")
> ## End(Not run)
> 
> 
> 
> cleanEx()
> nameEx("errorest")
> ### * errorest
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: errorest
> ### Title: Estimators of Prediction Error
> ### Aliases: errorest errorest.data.frame errorest.default
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> # Classification
> 
> data("iris")
> library("MASS")
> 
> # force predict to return class labels only
> mypredict.lda <- function(object, newdata)
+   predict(object, newdata = newdata)$class
> 
> # 10-fold cv of LDA for Iris data
> errorest(Species ~ ., data=iris, model=lda, 
+          estimator = "cv", predict= mypredict.lda)

Call:
errorest.data.frame(formula = Species ~ ., data = iris, model = lda, 
    predict = mypredict.lda, estimator = "cv")

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.02 

> 
> data("PimaIndiansDiabetes", package = "mlbench")
> ## Not run: 
> ##D # 632+ bootstrap of LDA for Diabetes data
> ##D errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
> ##D          estimator = "632plus", predict= mypredict.lda)
> ## End(Not run)
> 
> #cv of a fixed partition of the data
> list.tindx <- list(1:100, 101:200, 201:300, 301:400, 401:500,
+         501:600, 601:700, 701:768)
> 
> errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
+           estimator = "cv", predict = mypredict.lda,
+           est.para = control.errorest(list.tindx = list.tindx))

Call:
errorest.data.frame(formula = diabetes ~ ., data = PimaIndiansDiabetes, 
    model = lda, predict = mypredict.lda, estimator = "cv", est.para = control.errorest(list.tindx = list.tindx))

	 8-fold cross-validation estimator of misclassification error 

Misclassification error:  0.2227 

> 
> ## Not run: 
> ##D #both bootstrap estimations based on fixed partitions
> ##D 
> ##D list.tindx <- vector(mode = "list", length = 25)
> ##D for(i in 1:25) {
> ##D   list.tindx[[i]] <- sample(1:768, 768, TRUE)
> ##D }
> ##D 
> ##D errorest(diabetes ~ ., data=PimaIndiansDiabetes, model=lda,
> ##D           estimator = c("boot", "632plus"), predict= mypredict.lda,
> ##D           est.para = control.errorest(list.tindx = list.tindx))
> ##D 
> ## End(Not run)
> data("Glass", package = "mlbench")
> 
> # LDA has cross-validated misclassification error of
> # 38% (Ripley, 1996, page 98)
> 
> # Pruned trees about 32% (Ripley, 1996, page 230)
> 
> # use stratified sampling here, i.e. preserve the class proportions
> errorest(Type ~ ., data=Glass, model=lda, 
+          predict=mypredict.lda, est.para=control.errorest(strat=TRUE))

Call:
errorest.data.frame(formula = Type ~ ., data = Glass, model = lda, 
    predict = mypredict.lda, est.para = control.errorest(strat = TRUE))

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.3785 

> 
> # force predict to return class labels
> mypredict.rpart <- function(object, newdata)
+   predict(object, newdata = newdata,type="class")
> 
> library("rpart")
> pruneit <- function(formula, ...)
+   prune(rpart(formula, ...), cp =0.01)
> 
> errorest(Type ~ ., data=Glass, model=pruneit,
+          predict=mypredict.rpart, est.para=control.errorest(strat=TRUE))

Call:
errorest.data.frame(formula = Type ~ ., data = Glass, model = pruneit, 
    predict = mypredict.rpart, est.para = control.errorest(strat = TRUE))

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.3178 

> 
> # compute sensitivity and specifity for stabilised LDA
> 
> data("GlaucomaM", package = "TH.data")
> 
> error <- errorest(Class ~ ., data=GlaucomaM, model=slda,
+   predict=mypredict.lda, est.para=control.errorest(predictions=TRUE))
> 
> # sensitivity 
> 
> mean(error$predictions[GlaucomaM$Class == "glaucoma"] == "glaucoma")
[1] 0.8163265
> 
> # specifity
> 
> mean(error$predictions[GlaucomaM$Class == "normal"] == "normal")
[1] 0.8367347
> 
> # Indirect Classification: Smoking data
> 
> data(Smoking)
> # Set three groups of variables:
> # 1) explanatory variables are: TarY, NicY, COY, Sex, Age
> # 2) intermediate variables are: TVPS, BPNL, COHB
> # 3) response (resp) is defined by:
> 
> resp <- function(data){
+   data <- data[, c("TVPS", "BPNL", "COHB")]
+   res <- t(t(data) > c(4438, 232.5, 58))
+   res <- as.factor(ifelse(apply(res, 1, sum) > 2, 1, 0))
+   res
+ }
> 
> response <- resp(Smoking[ ,c("TVPS", "BPNL", "COHB")])
> smoking <- cbind(Smoking, response)
> 
> formula <- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age
> 
> # Estimation per leave-one-out estimate for the misclassification is 
> # 36.36% (Hand et al., 2001), using indirect classification with 
> # linear models
> ## Not run: 
> ##D errorest(formula, data = smoking, model = inclass,estimator = "cv", 
> ##D          pFUN = list(list(model=lm, predict = mypredict.lm)), cFUN = resp,  
> ##D          est.para=control.errorest(k=nrow(smoking)))
> ## End(Not run)
> 
> # Regression
> 
> data("BostonHousing", package = "mlbench")
> 
> # 10-fold cv of lm for Boston Housing data
> errorest(medv ~ ., data=BostonHousing, model=lm,
+          est.para=control.errorest(random=FALSE))

Call:
errorest.data.frame(formula = medv ~ ., data = BostonHousing, 
    model = lm, est.para = control.errorest(random = FALSE))

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  5.877 

> 
> # the same, with "model" returning a function for prediction
> # instead of an object of class "lm"
> 
> mylm <- function(formula, data) {
+   mod <- lm(formula, data)
+   function(newdata) predict(mod, newdata)
+ }
> 
> errorest(medv ~ ., data=BostonHousing, model=mylm,
+ est.para=control.errorest(random=FALSE))

Call:
errorest.data.frame(formula = medv ~ ., data = BostonHousing, 
    model = mylm, est.para = control.errorest(random = FALSE))

	 10-fold cross-validation estimator of root mean squared error

Root mean squared error:  5.877 

> 
> 
> # Survival data
> 
> data("GBSG2", package = "TH.data")
> library("survival")
> 
> # prediction is fitted Kaplan-Meier
> predict.survfit <- function(object, newdata) object
> 
> # 5-fold cv of Kaplan-Meier for GBSG2 study
> errorest(Surv(time, cens) ~ 1, data=GBSG2, model=survfit,
+          predict=predict.survfit, est.para=control.errorest(k=5))

Call:
errorest.data.frame(formula = Surv(time, cens) ~ 1, data = GBSG2, 
    model = survfit, predict = predict.survfit, est.para = control.errorest(k = 5))

	 5-fold cross-validation estimator of Brier's score

Brier's score:  0.1927 

> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’, ‘package:rpart’, ‘package:MASS’

> nameEx("inbagg")
> ### * inbagg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inbagg
> ### Title: Indirect Bagging
> ### Aliases: inbagg inbagg.default inbagg.data.frame
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> library("MASS")
> library("rpart")
> y <- as.factor(sample(1:2, 100, replace = TRUE))
> W <- mvrnorm(n = 200, mu = rep(0, 3), Sigma = diag(3))
> X <- mvrnorm(n = 200, mu = rep(2, 3), Sigma = diag(3))
> colnames(W) <- c("w1", "w2", "w3") 
> colnames(X) <- c("x1", "x2", "x3") 
> DATA <- data.frame(y, W, X)
> 
> 
> pFUN <- list(list(formula = w1~x1+x2, model = lm, predict = mypredict.lm),
+ list(model = rpart))
> 
> inbagg(y~w1+w2+w3~x1+x2+x3, data = DATA, pFUN = pFUN)

 Indirect bagging, with 25 bootstrap samples and intermediate variables: 
 w1 w2 w3 
> 
> 
> 
> cleanEx()

detaching ‘package:rpart’, ‘package:MASS’

> nameEx("inclass")
> ### * inclass
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: inclass
> ### Title: Indirect Classification
> ### Aliases: inclass inclass.default inclass.data.frame
> ### Keywords: misc
> 
> ### ** Examples
> 
> data("Smoking", package = "ipred")
> # Set three groups of variables:
> # 1) explanatory variables are: TarY, NicY, COY, Sex, Age
> # 2) intermediate variables are: TVPS, BPNL, COHB
> # 3) response (resp) is defined by:
> 
> classify <- function(data){
+   data <- data[,c("TVPS", "BPNL", "COHB")]
+   res <- t(t(data) > c(4438, 232.5, 58))
+   res <- as.factor(ifelse(apply(res, 1, sum) > 2, 1, 0))
+   res
+ }
> 
> response <- classify(Smoking[ ,c("TVPS", "BPNL", "COHB")])
> smoking <- data.frame(Smoking, response)
> 
> formula <- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age
> 
> inclass(formula, data = smoking, pFUN = list(list(model = lm, predict =
+ mypredict.lm)), cFUN = classify)

 Indirect classification, with 3 intermediate variables: 
 TVPS BPNL COHB 
 
 Predictive model per intermediate is lm 
> 
> 
> 
> 
> cleanEx()
> nameEx("ipredknn")
> ### * ipredknn
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: ipredknn
> ### Title: k-Nearest Neighbour Classification
> ### Aliases: ipredknn
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> library("mlbench")
> learn <- as.data.frame(mlbench.twonorm(300))
> 
> mypredict.knn <- function(object, newdata) 
+                    predict.ipredknn(object, newdata, type="class")
> 
> errorest(classes ~., data=learn, model=ipredknn, 
+          predict=mypredict.knn)

Call:
errorest.data.frame(formula = classes ~ ., data = learn, model = ipredknn, 
    predict = mypredict.knn)

	 10-fold cross-validation estimator of misclassification error 

Misclassification error:  0.0533 

> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:mlbench’

> nameEx("kfoldcv")
> ### * kfoldcv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: kfoldcv
> ### Title: Subsamples for k-fold Cross-Validation
> ### Aliases: kfoldcv
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> # 10-fold CV with N = 91
> 
> kfoldcv(10, 91)	
 [1] 10  9  9  9  9  9  9  9  9  9
> 
> ## Don't show: 
> k <- sample(5:15, 1)
> k
[1] 7
> N <- sample(50:150, 1)
> N
[1] 87
> stopifnot(sum(kfoldcv(k, N)) == N)
> ## End(Don't show)
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.bagging")
> ### * predict.bagging
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.classbagg
> ### Title: Predictions from Bagging Trees
> ### Aliases: predict.classbagg predict.regbagg predict.survbagg
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
> data("Ionosphere", package = "mlbench")
> Ionosphere$V2 <- NULL # constant within groups
> 
> # nbagg = 10 for performance reasons here
> mod <- bagging(Class ~ ., data=Ionosphere)
> 
> # out-of-bag estimate
> 
> mean(predict(mod) != Ionosphere$Class)
[1] 0.07977208
> 
> # predictions for the first 10 observations
> 
> predict(mod, newdata=Ionosphere[1:10,])
 [1] good bad  good bad  good bad  good bad  good bad 
Levels: bad good
> 
> predict(mod, newdata=Ionosphere[1:10,], type="prob")
       bad good
 [1,] 0.00 1.00
 [2,] 1.00 0.00
 [3,] 0.00 1.00
 [4,] 0.64 0.36
 [5,] 0.00 1.00
 [6,] 1.00 0.00
 [7,] 0.00 1.00
 [8,] 1.00 0.00
 [9,] 0.00 1.00
[10,] 1.00 0.00
> 
> 
> 
> 
> cleanEx()
> nameEx("predict.inbagg")
> ### * predict.inbagg
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.inbagg
> ### Title: Predictions from an Inbagg Object
> ### Aliases: predict.inbagg
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> library("MASS")
> library("rpart")
> y <- as.factor(sample(1:2, 100, replace = TRUE))
> W <- mvrnorm(n = 200, mu = rep(0, 3), Sigma = diag(3)) 
> X <- mvrnorm(n = 200, mu = rep(2, 3), Sigma = diag(3))
> colnames(W) <- c("w1", "w2", "w3")
> colnames(X) <- c("x1", "x2", "x3")
> DATA <- data.frame(y, W, X)
> 
> pFUN <- list(list(formula = w1~x1+x2, model = lm),
+ list(model = rpart))
> 
> RES <- inbagg(y~w1+w2+w3~x1+x2+x3, data = DATA, pFUN = pFUN)
> predict(RES, newdata = X)
  [1] 1 1 2 2 1 2 2 2 2 1 1 1 2 1 2 1 2 2 1 2 2 1 2 1 1 1 1 1 2 1 1 2 1 1 2 2 2
 [38] 1 2 1 2 2 2 2 2 2 1 1 2 2 1 2 1 1 1 1 1 2 2 1 2 1 1 1 2 1 1 2 1 2 1 2 1 1
 [75] 1 2 2 1 2 2 1 2 1 1 2 1 2 1 1 1 1 1 2 2 2 2 1 1 2 2 1 1 2 2 1 2 2 2 2 1 1
[112] 1 2 1 2 1 2 2 1 2 2 1 2 1 1 1 1 1 2 1 1 2 1 1 2 2 2 1 2 1 2 2 2 2 2 2 1 1
[149] 2 2 1 2 1 1 1 1 1 2 2 1 2 1 1 1 2 1 1 2 1 2 1 2 1 1 1 2 2 1 2 2 1 2 1 1 2
[186] 1 2 1 1 1 1 1 2 2 2 2 1 1 2 2
Levels: 1 2
> 
> 
> 
> cleanEx()

detaching ‘package:rpart’, ‘package:MASS’

> nameEx("predict.inclass")
> ### * predict.inclass
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: predict.inclass
> ### Title: Predictions from an Inclass Object
> ### Aliases: predict.inclass
> ### Keywords: misc
> 
> ### ** Examples
> 
> ## Not run: 
> ##D # Simulation model, classification rule following Hand et al. (2001)
> ##D 
> ##D theta90 <- varset(N = 1000, sigma = 0.1, theta = 90, threshold = 0)
> ##D 
> ##D dataset <- as.data.frame(cbind(theta90$explanatory, theta90$intermediate))
> ##D names(dataset) <- c(colnames(theta90$explanatory),
> ##D colnames(theta90$intermediate))
> ##D 
> ##D classify <- function(Y, threshold = 0) {
> ##D   Y <- Y[,c("y1", "y2")]
> ##D   z <- (Y > threshold)
> ##D   resp <- as.factor(ifelse((z[,1] + z[,2]) > 1, 1, 0))
> ##D   return(resp)
> ##D }
> ##D 
> ##D formula <- response~y1+y2~x1+x2
> ##D 
> ##D fit <- inclass(formula, data = dataset, pFUN = list(list(model = lm)), 
> ##D  cFUN = classify)
> ##D 
> ##D predict(object = fit, newdata = dataset)
> ##D 
> ##D 
> ##D data("Smoking", package = "ipred")
> ##D 
> ##D # explanatory variables are: TarY, NicY, COY, Sex, Age
> ##D # intermediate variables are: TVPS, BPNL, COHB
> ##D # reponse is defined by:
> ##D 
> ##D classify <- function(data){
> ##D   data <- data[,c("TVPS", "BPNL", "COHB")]
> ##D   res <- t(t(data) > c(4438, 232.5, 58))
> ##D   res <- as.factor(ifelse(apply(res, 1, sum) > 2, 1, 0))
> ##D   res
> ##D }
> ##D 
> ##D response <- classify(Smoking[ ,c("TVPS", "BPNL", "COHB")])
> ##D smoking <- cbind(Smoking, response)
> ##D 
> ##D formula <- response~TVPS+BPNL+COHB~TarY+NicY+COY+Sex+Age
> ##D 
> ##D fit <- inclass(formula, data = smoking, 
> ##D   pFUN = list(list(model = lm)), cFUN = classify)
> ##D 
> ##D 
> ##D predict(object = fit, newdata = smoking)
> ## End(Not run)
> 
> data("GlaucomaMVF", package = "ipred")
> library("rpart")
> glaucoma <- GlaucomaMVF[,(names(GlaucomaMVF) != "tension")]
> # explanatory variables are derived by laser scanning image and intra occular pressure
> # intermediate variables are: clv, cs, lora
> # response is defined by
> 
> classify <- function (data) {
+   attach(data) 
+   res <- ifelse((!is.na(clv) & !is.na(lora) & clv >= 5.1 & lora >= 
+         49.23372) | (!is.na(clv) & !is.na(lora) & !is.na(cs) & 
+         clv < 5.1 & lora >= 58.55409 & cs < 1.405) | (is.na(clv) & 
+         !is.na(lora) & !is.na(cs) & lora >= 58.55409 & cs < 1.405) | 
+         (!is.na(clv) & is.na(lora) & cs < 1.405), 0, 1)
+   detach(data)
+   factor (res, labels = c("glaucoma", "normal"))
+ }
> 
> fit <- inclass(Class~clv+lora+cs~., data = glaucoma, 
+              pFUN = list(list(model = rpart)), cFUN = classify)
> 
> data("GlaucomaM", package = "TH.data")
> predict(object = fit, newdata = GlaucomaM)
  [1] normal   normal   normal   normal   glaucoma glaucoma normal   normal  
  [9] normal   normal   normal   normal   glaucoma normal   normal   normal  
 [17] normal   normal   normal   glaucoma normal   normal   glaucoma normal  
 [25] normal   normal   glaucoma normal   glaucoma normal   normal   normal  
 [33] normal   normal   normal   normal   glaucoma normal   normal   normal  
 [41] normal   normal   glaucoma normal   normal   glaucoma normal   normal  
 [49] normal   normal   normal   glaucoma glaucoma glaucoma glaucoma normal  
 [57] glaucoma glaucoma normal   normal   glaucoma normal   glaucoma normal  
 [65] glaucoma normal   normal   normal   normal   normal   glaucoma glaucoma
 [73] glaucoma normal   normal   normal   glaucoma normal   normal   normal  
 [81] glaucoma normal   normal   normal   normal   glaucoma glaucoma glaucoma
 [89] glaucoma normal   normal   normal   glaucoma normal   normal   normal  
 [97] normal   normal   glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[105] normal   glaucoma normal   glaucoma glaucoma glaucoma glaucoma glaucoma
[113] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[121] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[129] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[137] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[145] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[153] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[161] glaucoma glaucoma glaucoma normal   glaucoma glaucoma glaucoma glaucoma
[169] glaucoma glaucoma glaucoma glaucoma normal   glaucoma glaucoma glaucoma
[177] glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma glaucoma
[185] glaucoma glaucoma normal   glaucoma glaucoma glaucoma glaucoma glaucoma
[193] glaucoma glaucoma glaucoma glaucoma
Levels: glaucoma normal
> 
> 
> 
> 
> cleanEx()

detaching ‘package:rpart’

> nameEx("prune.bagging")
> ### * prune.bagging
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: prune.classbagg
> ### Title: Pruning for Bagging
> ### Aliases: prune.classbagg prune.regbagg prune.survbagg
> ### Keywords: tree
> 
> ### ** Examples
> 
> 
> data("Glass", package = "mlbench")
> library("rpart")
> 
> mod <- bagging(Type ~ ., data=Glass, nbagg=10, coob=TRUE)
> pmod <- prune(mod)
> print(pmod)

Bagging classification trees with 10 bootstrap replications 

Call: bagging.data.frame(formula = Type ~ ., data = Glass, nbagg = 10, 
    coob = TRUE)

Out-of-bag estimate of misclassification error:  0.285 

> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:rpart’

> nameEx("rsurv")
> ### * rsurv
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: rsurv
> ### Title: Simulate Survival Data
> ### Aliases: rsurv
> ### Keywords: survival
> 
> ### ** Examples
> 
> 
> library("survival")
> # 3*X1 + X2
> simdat <- rsurv(500, model="C")
> coxph(Surv(time, cens) ~ ., data=simdat)
Call:
coxph(formula = Surv(time, cens) ~ ., data = simdat)

      coef exp(coef) se(coef)     z       p
X1  3.1555   23.4648   0.2023 15.60 < 2e-16
X2  1.1015    3.0086   0.1628  6.77 1.3e-11
X3 -0.2103    0.8104   0.1525 -1.38   0.168
X4  0.0466    1.0477   0.1488  0.31   0.754
X5  0.2709    1.3111   0.1536  1.76   0.078

Likelihood ratio test=289  on 5 df, p=0
n= 500, number of events= 500 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("sbrier")
> ### * sbrier
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: sbrier
> ### Title: Model Fit for Survival Data
> ### Aliases: sbrier
> ### Keywords: survival
> 
> ### ** Examples
> 
> 
> library("survival")
> data("DLBCL", package = "ipred")
> smod <- Surv(DLBCL$time, DLBCL$cens)
> 
> KM <- survfit(smod ~ 1)
> # integrated Brier score up to max(DLBCL$time)
> sbrier(smod, KM)
          [,1]
[1,] 0.2237226
attr(,"names")
[1] "integrated Brier score"
attr(,"time")
[1]   1.3 129.9
> 
> # integrated Brier score up to time=50
> sbrier(smod, KM, btime=c(0, 50))
Warning in sbrier(smod, KM, btime = c(0, 50)) :
  btime[1] is smaller than min(time)
          [,1]
[1,] 0.2174081
attr(,"names")
[1] "integrated Brier score"
attr(,"time")
[1]  1.3 39.6
> 
> # Brier score for time=50
> sbrier(smod, KM, btime=50)
Brier score 
   0.249375 
attr(,"time")
[1] 50
> 
> # a "real" model: one single survival tree with Intern. Prognostic Index
> # and mean gene expression in the first cluster as predictors
> mod <- bagging(Surv(time, cens) ~ MGEc.1 + IPI, data=DLBCL, nbagg=1)
> 
> # this is a list of survfit objects (==KM-curves), one for each observation
> # in DLBCL
> pred <- predict(mod, newdata=DLBCL)
> 
> # integrated Brier score up to max(time)
> sbrier(smod, pred)
          [,1]
[1,] 0.1442559
attr(,"names")
[1] "integrated Brier score"
attr(,"time")
[1]   1.3 129.9
> 
> # Brier score at time=50
> sbrier(smod, pred, btime=50)
Brier score 
  0.1774478 
attr(,"time")
[1] 50
> # artificial examples and illustrations
> 
> cleans <- function(x) { attr(x, "time") <- NULL; names(x) <- NULL; x }
> 
> n <- 100
> time <- rpois(n, 20)
> cens <- rep(1, n)
> 
> # checks, Graf et al. page 2536, no censoring at all!
> # no information: \pi(t) = 0.5 
> 
> a <- sbrier(Surv(time, cens), rep(0.5, n), time[50])
> stopifnot(all.equal(cleans(a),0.25))
> 
> # some information: \pi(t) = S(t)
> 
> n <- 100
> time <- 1:100
> mod <- survfit(Surv(time, cens) ~ 1)
> a <- sbrier(Surv(time, cens), rep(list(mod), n))
> mymin <- mod$surv * (1 - mod$surv)
> cleans(a)
          [,1]
[1,] 0.1682833
> sum(mymin)/diff(range(time))
[1] 0.1683333
> 
> # independent of ordering
> rand <- sample(1:100)
> b <- sbrier(Surv(time, cens)[rand], rep(list(mod), n)[rand])
> stopifnot(all.equal(cleans(a), cleans(b)))
> 
> ## Don't show: 
>   # total information: \pi(t | X) known for every obs
> 
>   time <- 1:10
>   cens <- rep(1,10)
>   pred <- diag(10)
>   pred[upper.tri(pred)] <- 1
>   diag(pred) <- 0
>   # <FIXME>
>   # a <- sbrier(Surv(time, cens), pred)
>   # stopifnot(all.equal(a, 0))
>   # </FIXME>
> ## End(Don't show)
> 
> # 2 groups at different risk
> 
> time <- c(1:10, 21:30)
> strata <- c(rep(1, 10), rep(2, 10))
> cens <- rep(1, length(time))
> 
> # no information about the groups
> 
> a <- sbrier(Surv(time, cens), survfit(Surv(time, cens) ~ 1))
> b <- sbrier(Surv(time, cens), rep(list(survfit(Surv(time, cens) ~1)), 20))
> stopifnot(all.equal(a, b))
> 
> # risk groups known
> 
> mod <- survfit(Surv(time, cens) ~ strata)
> b <- sbrier(Surv(time, cens), c(rep(list(mod[1]), 10), rep(list(mod[2]), 10)))
> stopifnot(a > b)
> 
> ### GBSG2 data
> data("GBSG2", package = "TH.data")
> 
> thsum <- function(x) {
+   ret <- c(median(x), quantile(x, 0.25), quantile(x,0.75))
+   names(ret)[1] <- "Median"
+   ret
+ }
> 
> t(apply(GBSG2[,c("age", "tsize", "pnodes",
+                  "progrec", "estrec")], 2, thsum))
        Median 25%    75%
age       53.0  46  61.00
tsize     25.0  20  35.00
pnodes     3.0   1   7.00
progrec   32.5   7 131.75
estrec    36.0   8 114.00
> 
> table(GBSG2$menostat)

 Pre Post 
 290  396 
> table(GBSG2$tgrade)

  I  II III 
 81 444 161 
> table(GBSG2$horTh)

 no yes 
440 246 
> 
> # pooled Kaplan-Meier
> 
> mod <- survfit(Surv(time, cens) ~ 1, data=GBSG2)
> # integrated Brier score
> sbrier(Surv(GBSG2$time, GBSG2$cens), mod)
          [,1]
[1,] 0.1939366
attr(,"names")
[1] "integrated Brier score"
attr(,"time")
[1]    8 2659
> # Brier score at 5 years
> sbrier(Surv(GBSG2$time, GBSG2$cens), mod, btime=1825)
Brier score 
  0.2499984 
attr(,"time")
[1] 1825
> 
> # Nottingham prognostic index
> 
> GBSG2 <- GBSG2[order(GBSG2$time),]
> 
> NPI <- 0.2*GBSG2$tsize/10 + 1 + as.integer(GBSG2$tgrade)
> NPI[NPI < 3.4] <- 1
> NPI[NPI >= 3.4 & NPI <=5.4] <- 2
> NPI[NPI > 5.4] <- 3
> 
> mod <- survfit(Surv(time, cens) ~ NPI, data=GBSG2)
> plot(mod)
> 
> pred <- c()
> survs <- c()
> for (i in sort(unique(NPI)))
+     survs <- c(survs, getsurv(mod[i], 1825))
> 
> for (i in 1:nrow(GBSG2))
+    pred <- c(pred, survs[NPI[i]])
> 
> # Brier score of NPI at t=5 years
> sbrier(Surv(GBSG2$time, GBSG2$cens), pred, btime=1825)
Brier score 
   0.233823 
attr(,"time")
[1] 1825
> 
> 
> 
> 
> 
> cleanEx()

detaching ‘package:survival’

> nameEx("slda")
> ### * slda
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: slda
> ### Title: Stabilised Linear Discriminant Analysis
> ### Aliases: slda slda.default slda.formula slda.factor
> ### Keywords: multivariate
> 
> ### ** Examples
> 
> 
> library("mlbench")
> library("MASS")
> learn <- as.data.frame(mlbench.twonorm(100))
> test <- as.data.frame(mlbench.twonorm(1000))
> 
> mlda <- lda(classes ~ ., data=learn)
> mslda <- slda(classes ~ ., data=learn)
> 
> print(mean(predict(mlda, newdata=test)$class != test$classes))
[1] 0.047
> print(mean(predict(mslda, newdata=test)$class != test$classes))
[1] 0.025
> 
> 
> 
> 
> cleanEx()

detaching ‘package:MASS’, ‘package:mlbench’

> nameEx("varset")
> ### * varset
> 
> flush(stderr()); flush(stdout())
> 
> ### Name: varset
> ### Title: Simulation Model
> ### Aliases: varset
> ### Keywords: misc
> 
> ### ** Examples
> 
> 
> theta90 <- varset(N = 1000, sigma = 0.1, theta = 90, threshold = 0)
> theta0 <- varset(N = 1000, sigma = 0.1, theta = 0, threshold = 0)
> par(mfrow = c(1, 2))
> plot(theta0$intermediate)
> plot(theta90$intermediate)
> 
> 
> 
> 
> graphics::par(get("par.postscript", pos = 'CheckExEnv'))
> ### * <FOOTER>
> ###
> options(digits = 7L)
> base::cat("Time elapsed: ", proc.time() - base::get("ptime", pos = 'CheckExEnv'),"\n")
Time elapsed:  4.548 0.04 4.606 0 0 
> grDevices::dev.off()
null device 
          1 
> ###
> ### Local variables: ***
> ### mode: outline-minor ***
> ### outline-regexp: "\\(> \\)?### [*]+" ***
> ### End: ***
> quit('no')
